# -*- coding: utf-8 -*-
"""
Created on Wed Dec 29 11:45:19 2021
@author: Administrator
Description:
(1) MT-RF is generated by joint modeling of training sets of all endpoints.
(2) For each endpoint, iteratively finds hard sample set and generates a set of ECDTs.
(3) In prediction process, the combination of MT-RF and  the corresponding ECDTs is used for predicting
 the testing set of each endpoints.
"""
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import KFold
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from math import sqrt
from sklearn import tree
from sklearn.neighbors import NearestNeighbors
import sys
import joblib
import time
sys.path.insert(0, "../")
start_time=time.time()

#******parameter setting starts******
test_data   = "[1]"
train_data1 = "[0]"
train_data2 = "[2]"
train_data3 = "[3]"
train_data4 = "[4]"

start=1
end=60
tree_amount=60
K=1  #Find K neighbors for the hard samples
seed=42
threshold=0.002
feature_type="Avalon"

print("Feature type:", feature_type)
print("Num of endpoints=",end-1)
#print("Initial number of decision trees in MT-RF=",tree_amount)
print("Fold sequence:", test_data, train_data1, train_data2, train_data3, train_data4)
print("K=",K)
print("threshold=", threshold)

#******load all datasets starts*****
for i in range(start,end):
    path_feature='/home/gaoj/01JKY_toxicity_fromWu/DataAndCode/Feature/' + feature_type + '/' + feature_type + '_' + str(i) +'.csv'
    path_label='/home/gaoj/01JKY_toxicity_fromWu/DataAndCode/Label/Label_'+str(i)+'.csv'
    #load feature
    sentence = "X"+str(i)+ "= pd.read_csv(path_feature)"
    exec(sentence)
    sentence = "X"+str(i)+ "= np.array(X"+str(i)+".values[:,1:])"
    exec(sentence)
    #load label
    sentence = "Y"+str(i)+ "= pd.read_csv(path_label)"
    exec(sentence)    
    sentence="Y"+str(i)+"= np.array(Y"+str(i)+".values[:,-1])"
    exec(sentence)
    #finish load i-th data
    print(i,"-th dataset has already loaded")
    sentence="print(X" + str(i) + ".shape,Y"+str(i)+".shape)"
    exec(sentence)  

#******Each dataset is divided into 5 folds*****
def get_cross_validation_data(X,Y):    
    data=[];label=[]
    kf = KFold(n_splits=5, shuffle=True, random_state=seed)
    for train_index, test_index in kf.split(X):
        data.append(X[test_index])
        label.append(Y[test_index])        
    return data,label

for i in range(start,end):
    sentence="data"+str(i)+",label"+str(i)+"=get_cross_validation_data(X"+str(i)+",Y"+str(i)+")"
    exec(sentence)
    #print("data ",i," split successfully")
   
#******Find hard samples and their K-neighbors*******
def find_Hard_Samples_and_k_neighbor(ensemble, X_train, Y_train, k_neighbours): 
    y_pred=[] 
    for tree1 in ensemble:
        y_pred.append(tree1.predict(X_train)) 
    temp=np.zeros(len(y_pred[0])) 
    for e in y_pred:
        temp=temp+np.array(e) 
    y_pred=temp/len(ensemble) 
    distance= np.abs(y_pred-Y_train)
    threshold=np.mean(distance)
    distance=distance>threshold
    hard_sample_index=[]
    for i in range(len(distance)):
        if distance[i]==1:
            hard_sample_index.append(i) 
    hard_sample_set=X_train[hard_sample_index] 
    neigh = NearestNeighbors(n_neighbors=k_neighbours)
    neigh.fit(X_train)
    dis,nei = neigh.kneighbors(hard_sample_set) 
    hard_sample_index_for_tree=[]
    for element1 in nei:
        for element2 in element1:
            hard_sample_index_for_tree.append(element2)
    x_tree = X_train[hard_sample_index_for_tree]    
    y_tree = Y_train[hard_sample_index_for_tree]
    return x_tree,y_tree

#******Evaluate the r2 metric after MF-RF adding a new ECDT*******
def evaluate_r2_performance_for_estimator_list(ensemble, X_test, Y_test):
    y_pred=[]
    for tree1 in ensemble:
        y_pred.append(tree1.predict(X_test)) 
    a=np.zeros(len(y_pred[0])) 
    for e in y_pred:
        a=a+np.array(e) 
    a=a/len(ensemble) 
    r2=r2_score(Y_test,a)
    mse=mean_squared_error(Y_test,a)
    rmse=sqrt(mse)
    mae=mean_absolute_error(Y_test,a)
    return r2,rmse,mae

#******Prediction process of the proposed model*******
def prediction_of_estimator_list(ensemble, X_test):
    y_pred=[]
    for tree1 in ensemble:
        y_pred.append(tree1.predict(X_test)) 
    a=np.zeros(len(y_pred[0])) 
    for e in y_pred:
        a=a+np.array(e) 
    a=a/len(ensemble)
    a=list(a)
    a=[round(i,3) for i in a]
    return list(a)

#******Main program*******
X_train=[]
Y_train=[]    
for i in range(start,end):    
    sentence="X"+str(i)+"_test = data"+str(i) + test_data
    exec(sentence)    
    sentence="Y"+str(i)+"_test = label"+str(i)+ test_data
    exec(sentence)
    #Generate the testing set of the i-th dataset, accounting for 20%    
    sentence="fold1_x=list(data"+str(i) + train_data1 + ")"
    exec(sentence)
    sentence="fold2_x=list(data"+str(i) + train_data2 + ")"
    exec(sentence)
    sentence="fold3_x=list(data"+str(i) + train_data3 + ")"
    exec(sentence)
    sentence="fold4_x=list(data"+str(i) + train_data4 + ")"
    exec(sentence)
    sentence="fold1_y=list(label"+str(i)+ train_data1 + ")"
    exec(sentence)
    sentence="fold2_y=list(label"+str(i)+ train_data2 + ")"
    exec(sentence)
    sentence="fold3_y=list(label"+str(i)+ train_data3 + ")"
    exec(sentence)
    sentence="fold4_y=list(label"+str(i)+ train_data4 + ")"
    exec(sentence)
    x_temp=fold1_x + fold2_x + fold3_x + fold4_x 
    y_temp=fold1_y + fold2_y + fold3_y + fold4_y
    #The remaining 80% of i-th dataset is merged together for training
    sentence="X"+str(i)+"_train=np.array(x_temp)"
    exec(sentence)
    sentence="Y"+str(i)+"_train=np.array(y_temp)"
    exec(sentence)
    X_train=X_train + x_temp
    Y_train=Y_train + y_temp

print("Shape of traing set for MT-RF：",np.array(X_train).shape)
rf=RandomForestRegressor(n_estimators=tree_amount, random_state=seed, n_jobs=-1) #n_estimators=tree_amount,random_state=seed
rf.fit(X_train,Y_train)
print("Num of trees in MT-RF:",len(rf.estimators_))

rf_for_all_tasks=[]
for i in range(start,end): 
    sentence="r2=rf.score(X"+str(i)+"_train,Y"+str(i)+"_train)"
    exec(sentence) 
    ensemble_temp =rf.estimators_.copy()    
    ensemble_final=rf.estimators_.copy()
    #print(len(ensemble_temp))
    #print(len(ensemble_final))
    p=1
    while p:
        sentence="x_tree,y_tree=find_Hard_Samples_and_k_neighbor(ensemble_temp, X"+str(i)+"_train, Y"+str(i)+"_train," +str(K)+ ")"
        exec(sentence)
        clf = tree.DecisionTreeRegressor()
        clf.fit(x_tree,y_tree)
        ensemble_temp.append(clf)
        sentence="new_r2,rmse,mae=evaluate_r2_performance_for_estimator_list(ensemble_temp, X"+str(i)+"_train, Y"+str(i)+"_train)"
        exec(sentence)        
        if new_r2 - r2 > threshold :
            ensemble_final.append(clf)
            r2=new_r2      
        else:       
            break
    rf_for_all_tasks.append(ensemble_final)
    print(i,"-th model finish training, Num of trees:",len(ensemble_final))

r2_fold1=[]
rmse_fold1=[]
mae_fold1=[]
result_metrix=[]
for i in range(start,end): 
    sentence="r2r,rmse,mae=evaluate_r2_performance_for_estimator_list(rf_for_all_tasks["+str(i-1)+"], X"+str(i)+"_test, Y"+str(i)+"_test)"
    exec(sentence) 
    r2_fold1.append(r2r)
    rmse_fold1.append(rmse)
    mae_fold1.append(mae)
    sentence="temp=prediction_of_estimator_list(rf_for_all_tasks["+str(i-1)+"], X"+str(i)+"_test)"
    exec(sentence)
    result_metrix.append(temp)

     
print("Avg r2   of all endpoints:", round(np.mean(r2_fold1),2))
print("Avg rmse of all endpoints:", round(np.mean(rmse_fold1),2))
print("Avg mae  of all endpoints:", round(np.mean(mae_fold1),2))
end_time=time.time()
hours = (end_time-start_time)/(60*60)
print("Time-consuming:", round(hours,3),"hours")